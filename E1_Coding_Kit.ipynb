{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnrudrapogu/CSC-Final-Project/blob/main/E1_Coding_Kit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# E1: Unimodal Language Representations (Coding Part)\n",
        "---\n",
        "Welcome to the coding part of the first exercise sheet. It will be about training `Transformer Language Models` (LMs) on text. It consists of six major components:\n",
        "* **Data Preparation**: For that, please download the `lotr.txt` file from Moodle or by clicking on the link in the exercise sheet. This file, which contains the text from `The Lord of the Rings`, will be used to train our LMs in this exercise. Upload this file to this Google Colab / Kaggle instance or alternatively to your Google Drive if you decide to mount your personal Drive.\n",
        "* **1 - Tokenization**: After the training text is ready, we have to think about how we want to tokenize it before training our model. As an initial approach to this, a `CharTokenizer` class is already provided, which implements a naive character-level tokenization method. Based on that, your task is to implement a data-driven `SubwordTokenizer` and accelerate it with a prefix tree data structure.\n",
        "* **2 -  Model Architecture**: In the second coding task, you will complete the already provided Transformer model definition.\n",
        "* **3 - Position Embeddings**: The provided model architecture does not include any sort of position embeddings. It's part of this exercise to implement a well-known type of positional embeddings.\n",
        "* **4 - Training and Evaluation**: After all the steps above, you will now be able to finally train your model(s)."
      ],
      "metadata": {
        "id": "hNlGsYgjjepg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ⚠️ **Note**: You do not need GPU acceleration until Task 4\n",
        "You can safely do the first three exercises without any GPU attached to your runtime."
      ],
      "metadata": {
        "id": "MeWjoC2-kjLD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMTLLRYwjNwM"
      },
      "outputs": [],
      "source": [
        "# Some necessary imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "import json\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "from dataclasses import dataclass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to your training text\n",
        "data_path = 'lotr.txt'\n",
        "\n",
        "# Optional:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_path = '/content/drive/MyDrive/MAI_SS25_Exercises/lotr.txt'\n",
        "\n",
        "# Load the training data\n",
        "with open(data_path, 'r', encoding='latin-1') as f:\n",
        "    lotr_text = f.read()\n",
        "\n",
        "print(f\"Dataset length: {len(lotr_text)} characters\")\n",
        "print(f\"Sample:\\n{lotr_text[:2000]}\")"
      ],
      "metadata": {
        "id": "E11XkUH6k-fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 - Tokenization\n",
        "The following `CharTokenizer` class implements the provided abstract (data-driven) Tokenizer interface. It is provided to you as a simple character-level tokenisation approach. It treats every character that occurs in the data as it's own token."
      ],
      "metadata": {
        "id": "WOKGnjq4lP7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer Interface\n",
        "class Tokenizer:\n",
        "\n",
        "    def __init__(self, text, vocab_size=None):\n",
        "        self.train(text, vocab_size)\n",
        "\n",
        "    def train(self, text):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def encode(self, s):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "# Character-level Tokenizer\n",
        "class CharTokenizer(Tokenizer):\n",
        "    def __init__(self, text):\n",
        "        super(CharTokenizer, self).__init__(text)\n",
        "\n",
        "    def train(self, text, vocab_size):\n",
        "        # Build vocab from sorted unique characters\n",
        "        self.vocab = sorted(list(set(text)))\n",
        "        self.token_to_id = {ch: i for i, ch in enumerate(self.vocab)}\n",
        "        self.id_to_token = {i: ch for ch, i in self.token_to_id.items()}\n",
        "\n",
        "    def encode(self, s):\n",
        "        \"\"\"Encode a string to a list of token indices\"\"\"\n",
        "        return [self.token_to_id[c] for c in s]\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        \"\"\"Decode a list of token indices back to a string\"\"\"\n",
        "        return ''.join([self.id_to_token[i] for i in tokens])\n",
        "\n",
        "# Instantiate tokenizer and process data\n",
        "tokenizer = CharTokenizer(lotr_text)\n",
        "data = torch.tensor(tokenizer.encode(lotr_text), dtype=torch.long)\n",
        "\n",
        "print(f\"Vocab size: {len(tokenizer.vocab)}\")\n",
        "print(f\"Vocab elements: {tokenizer.vocab}\")\n",
        "print(f\"Sample tokenization: 'Gandalf runs' -> {tokenizer.encode('Gandalf runs')}\")\n",
        "print(f\"Decoded back: {tokenizer.decode(tokenizer.encode('Gandalf runs'))}\")\n"
      ],
      "metadata": {
        "id": "EoGd1EOAlK05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a) 💻 Implement Subword Frequency Tokenization\n",
        "The `SubwordTokenizer` class shall do the following during training:\n",
        "\n",
        "\n",
        "1.   Start with a vocabulary $V$ that contains only all occurring characters.\n",
        "2.   Then repeat until `len(vocab)` is equal to the wanted `vocab_size`:\n",
        "\n",
        "*   Compute frequencies of token pairs in the text\n",
        "*   Take most frequent pair of tokens\n",
        "*   Merge them into a new token and update the vocabulary\n",
        "\n",
        "For encoding, it shall greedily take the longest matching token from the vocabulary as the next token in the encoded sequence. You are supposed to build a simple prefix tree after training to accelerate this search for the longest matching prefix token. This will modify the inner logic of your encode(...) method but significantly improve its performance. You may use the `pygtrie` package to implement the data structure. (`pip install pygtrie`)"
      ],
      "metadata": {
        "id": "75nUYdZglksh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the pygtrie package for the Prefix Trie data structure\n",
        "!pip install pygtrie"
      ],
      "metadata": {
        "id": "cpApHCTvmFMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pygtrie\n",
        "from collections import Counter\n",
        "\n",
        "# ================ STUDENT PART (START) ==================================\n",
        "\n",
        "class SubwordTokenizer(Tokenizer):\n",
        "\n",
        "    def __init__(self, text, vocab_size=1000):\n",
        "        super(SubwordTokenizer, self).__init__(text, vocab_size)\n",
        "\n",
        "    def train(self, text):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def encode(self, s):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        raise NotImplementedError\n",
        "\n",
        "# ================ STUDENT PART (END) =================================="
      ],
      "metadata": {
        "id": "trW5Ni_qmWG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we provide a PyTorch dataset class that holds the tokenized input and output sequences for us. Make sure to understand what it is doing before moving on."
      ],
      "metadata": {
        "id": "pcdlJX4Wmvn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextTokenDataset(Dataset):\n",
        "    def __init__(self, encoded_text, seq_length):\n",
        "        self.data = encoded_text\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.seq_length  # Total possible sequences\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_sequence = self.data[idx: idx + self.seq_length]  # input sequence of tokens\n",
        "        target_sequence = self.data[idx + 1: idx + self.seq_length + 1]  # target sequence (shifted by 1)\n",
        "\n",
        "        return torch.tensor(input_sequence), torch.tensor(target_sequence)"
      ],
      "metadata": {
        "id": "njq8t9yEmpMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 - Model Architecture\n",
        "Below you can find an almost complete PyTorch model that we will use as our LM. It consists of `TransformerBlock` modules."
      ],
      "metadata": {
        "id": "0xTVbF-YnJ8g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### a) 💻 Complete the TransformerBlock class\n",
        "The `TransformerBlock` class is currently incomplete; it is your task to implement its `forward` method. Make sure that you create and apply a causal mask for the attention mechanism."
      ],
      "metadata": {
        "id": "dbPqqCh5nNXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # Multi-head attention layer\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "\n",
        "        # Feedforward layers\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embed_dim, embed_dim)\n",
        "        )\n",
        "\n",
        "        # Normalization\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x, is_training=True):\n",
        "\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        # ============== STUDENT PART (START) ==================================\n",
        "\n",
        "        # ================ STUDENT PART (END) ==================================\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "8t4e9_ySnKQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b) 💻 Complete the TransformerModel class\n",
        "After that, complete the `forward` method of the `TransformerModel` class as well. This method combines the (optional) positional embedding, which we will implement later, and the token embeddings by an element-wise sum before passing them through the blocks and final layer."
      ],
      "metadata": {
        "id": "e2PZNPDgnSar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim=128, num_heads=4, n_blocks=2, context_length=128, pos_emb_class=None):\n",
        "\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.token_embed = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        if pos_emb_class is not None:\n",
        "            self.pos_emb = pos_emb_class(embed_dim, max_len=context_length)\n",
        "        else:\n",
        "            self.pos_emb = None\n",
        "\n",
        "        self.blocks = nn.ModuleList([TransformerBlock(embed_dim, num_heads) for _ in range(n_blocks)])\n",
        "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, idx, is_training=True):\n",
        "\n",
        "        # ============== STUDENT PART (START) ==================================\n",
        "\n",
        "        # ================ STUDENT PART (END) ==================================\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "Lqpx2jpGnRqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 - Position Embeddings\n",
        "Transformers don't have any built-in notion of sequence order like RNNs or CNNs. To help the model understand the order of tokens in a sequence, we add position embeddings to the token embeddings. These tell the model where in the sequence each token is."
      ],
      "metadata": {
        "id": "Tra1kGzHnf1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a) 💻 Implement a PositionEmbeddings class\n",
        "Implement the `SinusoidalPositionEmbeddings` class, which is the type of positional encoding used and described in [Attention is All You Need](https://arxiv.org/abs/1706.03762) (Vaswani et al. 2017).\n",
        "\n",
        "The final output of this PyTorch module for an input of `x.shape = (batch_size, seq_len, _)` should be of shape `(1, seq_len, embed_dim)`."
      ],
      "metadata": {
        "id": "d5hgFIOTnjAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_dim, max_len=512):\n",
        "        super(SinusoidalPositionEmbeddings, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # ============== STUDENT PART (START) ==================================\n",
        "\n",
        "        # ================ STUDENT PART (END) ==================================\n",
        "\n",
        "        return ...\n"
      ],
      "metadata": {
        "id": "Rt9bT2T9ngs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### c) 💻 Implement a helpful visualisation of your implemented positional embeddings.\n",
        "Be creative and include the result together with an explanation in your report."
      ],
      "metadata": {
        "id": "wAMo3IP0qqwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============== STUDENT PART (START) ==================================\n",
        "\n",
        "# ================ STUDENT PART (END) =================================="
      ],
      "metadata": {
        "id": "J_JWxMHTqrFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ⚠️ **Note**: You should now restart your runtime with GPU access."
      ],
      "metadata": {
        "id": "p9mstF6Z_sXo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 - Training and Evaluation\n",
        "Now it is finally time to train and also evaluate your language model. The training and sampling functions are already provided:"
      ],
      "metadata": {
        "id": "5WI2ynsJn0zO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, criterion, tokenizer, context_length, num_epochs=5):\n",
        "    n_batches_between_validation = 100\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", ncols=100)\n",
        "\n",
        "        for batch_idx, (input_seq, target_seq) in enumerate(loop):\n",
        "            input_seq, target_seq = input_seq.cuda(), target_seq.cuda()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(input_seq)\n",
        "\n",
        "            # Flatten logits and target sequences for loss computation\n",
        "            logits = logits.view(-1, logits.size(-1))  # Reshape for loss\n",
        "            target_seq = target_seq.view(-1)  # Flatten the target sequence\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(logits, target_seq)\n",
        "            loss.backward()\n",
        "\n",
        "            # Update weights\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Update the progress bar with the loss\n",
        "            loop.set_postfix(loss=running_loss / (batch_idx + 1))\n",
        "\n",
        "            if batch_idx % n_batches_between_validation == 0:\n",
        "                model.eval()\n",
        "                prompt = \"\"\"Gandalf \"\"\"\n",
        "                generated_tokens = generate(model, prompt, max_len=context_length)\n",
        "                model.train()\n",
        "                print(\" -> \" + tokenizer.decode(generated_tokens[0].tolist()))"
      ],
      "metadata": {
        "id": "kPo0THhlnmue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, prompt, max_len=128, temperature=1.0):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    prompt_tokens = torch.tensor(tokenizer.encode(prompt), dtype=torch.long).unsqueeze(0).cuda()\n",
        "    generated = prompt_tokens\n",
        "\n",
        "    for _ in range(max_len - prompt_tokens.shape[1]):\n",
        "        # Feed the current sequence into the model\n",
        "        logits = model(generated, is_training=False)\n",
        "\n",
        "        # Only get the logits for the last token (the token being predicted)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Apply temperature (optional, can control randomness of predictions)\n",
        "        logits = logits / temperature\n",
        "\n",
        "        # Sample from the probability distribution\n",
        "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "        next_token = torch.multinomial(probs, 1)  # Sample from the distribution\n",
        "\n",
        "        # Append the generated token to the sequence\n",
        "        generated = torch.cat([generated, next_token], dim=1)\n",
        "\n",
        "    return generated"
      ],
      "metadata": {
        "id": "V5rV6r5BoJU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 💻 a) Train your tokenizers and inspect their learned vocabularies"
      ],
      "metadata": {
        "id": "qq85Knw0oQ58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============== STUDENT PART (START) ==================================\n",
        "\n",
        "char_tokenizer = ...\n",
        "\n",
        "subword_tokenizer = ...\n",
        "\n",
        "# ================ STUDENT PART (END) =================================="
      ],
      "metadata": {
        "id": "DBHwpPCvoQeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============== STUDENT PART (START) ==================================\n",
        "\n",
        "# Tokenizer inspection here\n",
        "\n",
        "# ================ STUDENT PART (END) =================================="
      ],
      "metadata": {
        "id": "2xGUm9Z1oW8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Provided default configuration"
      ],
      "metadata": {
        "id": "EnRpuXpnokHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class DefaultConfig:\n",
        "\n",
        "  context_length: int = 64\n",
        "  embed_dim: int = 64\n",
        "  n_heads: int = 8\n",
        "  n_blocks: int = 4\n",
        "  ffn_dim: int = 512\n",
        "  batch_size: int = 64\n",
        "\n",
        "  n_epochs: int = 1\n",
        "  weight_decay: float = 0.01\n",
        "  learning_rate: float = 0.0005"
      ],
      "metadata": {
        "id": "9mKqcDl_oxwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Provided default setup"
      ],
      "metadata": {
        "id": "wthyVaPVpp1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose your tokenizer\n",
        "tokenizer = ...\n",
        "\n",
        "# Create the configuration\n",
        "config = DefaultConfig()\n",
        "vocab_size = len(tokenizer.vocab)\n",
        "\n",
        "# Encoding the entire text\n",
        "encoded_text = tokenizer.encode(lotr_text)\n",
        "train_dataset = TextTokenDataset(encoded_text, config.context_length)\n",
        "\n",
        "# Create a DataLoader to batch data\n",
        "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "\n",
        "# Create Model\n",
        "model = TransformerModel(\n",
        "      vocab_size,\n",
        "      config.embed_dim,\n",
        "      num_heads=config.n_heads,\n",
        "      n_blocks=config.n_blocks,\n",
        "      context_length=config.context_length,\n",
        "      pos_emb_class=SinusoidalPositionEmbeddings\n",
        "    ).cuda()\n",
        "\n",
        "# Print model size\n",
        "print(f\"Model size: {sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters\")\n",
        "\n",
        "# Optimizer and Loss function\n",
        "optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the model\n",
        "train(model, train_loader, optimizer, criterion, tokenizer, config.context_length, num_epochs=config.n_epochs)"
      ],
      "metadata": {
        "id": "kZHGki1zokAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation metric: Perplexity"
      ],
      "metadata": {
        "id": "zTHq87iUo9d7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perplexity(model, test_text, tokenizer, context_length):\n",
        "    model.eval()\n",
        "    test_encoded = tokenizer.encode(test_text)\n",
        "    test_dataset = TextTokenDataset(test_encoded, context_length)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_seq, target_seq in tqdm(test_loader):\n",
        "            input_seq, target_seq = input_seq.cuda(), target_seq.cuda()\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(input_seq)\n",
        "            logits = logits.view(-1, logits.size(-1))  # Reshape for loss\n",
        "            target_seq = target_seq.view(-1)  # Flatten the target sequence\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(logits, target_seq)\n",
        "            total_loss += loss.item()\n",
        "            total_tokens += target_seq.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    perplexity = math.exp(avg_loss)\n",
        "\n",
        "    return perplexity"
      ],
      "metadata": {
        "id": "Gjg1zwi_oj9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "value = perplexity(model, lotr_text[:len(lotr_text)//10], tokenizer, config.context_length)\n",
        "print(f\"Perplexity: {value:.2f}\")"
      ],
      "metadata": {
        "id": "lHI-CgPaojgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 💻 c) Train and evaluate your final models (obtained with the two tokenizers), w/ and /wo the positional embeddings (4 combinations). Stick to the default configuration for now."
      ],
      "metadata": {
        "id": "jpU9QrdrpVkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============== STUDENT PART (START) ==================================\n",
        "\n",
        "# Your experiment here. Ensure it reproduces the results in your report.\n",
        "\n",
        "# ================ STUDENT PART (END) =================================="
      ],
      "metadata": {
        "id": "RFvzoaOypeXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test your model with prompts"
      ],
      "metadata": {
        "id": "vBqWQTTBpEJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your model\n",
        "prompt = \"Gandalf\"\n",
        "generated_tokens = generate(model, prompt, max_len=config.context_length)\n",
        "\n",
        "print(tokenizer.decode(generated_tokens[0].tolist()))"
      ],
      "metadata": {
        "id": "EEd-WhPjpDjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Optional** (Only Karma Points)\n",
        "Generate a story about Gandalf with your best LM (*feel free to change the configuration*) and share it in the respective **Content Q&A forum** thread.\n"
      ],
      "metadata": {
        "id": "evuvbNRWqAt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============== STUDENT PART (START) ==================================\n",
        "\n",
        "prompt = \"...\"\n",
        "\n",
        "gandalfs_story = ...\n",
        "\n",
        "# ================ STUDENT PART (END) =================================="
      ],
      "metadata": {
        "id": "Ed8XESdoqAJT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}